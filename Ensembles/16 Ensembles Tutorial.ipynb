{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ensembles\n",
    "\n",
    "**Q1(a)**  \n",
    "Load the Wine dataset using the CSV file provided, and assess the accuracy of a decision tree classifier using 10-fold cross-validation.  \n",
    "What percentage of instances are correctly classified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Alcohol  Malic_acid   Ash  Alcalinity_of_ash  Magnesium  Total_phenols  \\\n0    14.23        1.71  2.43               15.6        127           2.80   \n1    13.20        1.78  2.14               11.2        100           2.65   \n2    13.16        2.36  2.67               18.6        101           2.80   \n3    14.37        1.95  2.50               16.8        113           3.85   \n4    13.24        2.59  2.87               21.0        118           2.80   \n\n   Flavanoids  Nonflavanoid_phenols  Proanthocyanins  Color_intensity   Hue  \\\n0        3.06                  0.28             2.29             5.64  1.04   \n1        2.76                  0.26             1.28             4.38  1.05   \n2        3.24                  0.30             2.81             5.68  1.03   \n3        3.49                  0.24             2.18             7.80  0.86   \n4        2.69                  0.39             1.82             4.32  1.04   \n\n   OD280/OD315_of_diluted_wines  Proline  class  \n0                          3.92     1065  Type1  \n1                          3.40     1050  Type1  \n2                          3.17     1185  Type1  \n3                          3.45     1480  Type1  \n4                          2.93      735  Type1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Alcohol</th>\n      <th>Malic_acid</th>\n      <th>Ash</th>\n      <th>Alcalinity_of_ash</th>\n      <th>Magnesium</th>\n      <th>Total_phenols</th>\n      <th>Flavanoids</th>\n      <th>Nonflavanoid_phenols</th>\n      <th>Proanthocyanins</th>\n      <th>Color_intensity</th>\n      <th>Hue</th>\n      <th>OD280/OD315_of_diluted_wines</th>\n      <th>Proline</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065</td>\n      <td>Type1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050</td>\n      <td>Type1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185</td>\n      <td>Type1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480</td>\n      <td>Type1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735</td>\n      <td>Type1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "wine_pd = pd.read_csv('Wine.csv')\n",
    "wine_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(178, 13)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = wine_pd.pop('class').values\n",
    "X = wine_pd.values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier(criterion='entropy')\n",
    "folds = 10\n",
    "v = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean for Tree 0.89\n"
     ]
    }
   ],
   "source": [
    "scores_tree = cross_val_score(dtree, X, y, cv=folds, verbose = v, n_jobs = -1)\n",
    "print(\"Mean for Tree {:.2f}\".format(scores_tree.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q1(b)**  \n",
    "Now, apply ensemble classification using bagging to achieve diversity and with a decision tree classifier.   \n",
    "What percentage of instances are now correctly classified with an ensemble of size 10? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wine_bag = BaggingClassifier(dtree,\n",
    "                            n_estimators = 10,\n",
    "                            max_samples = 1.0, # bootstrap resampling\n",
    "                            bootstrap = True)\n",
    "scores_bag = cross_val_score(wine_bag,X,y,cv= folds,verbose= v, n_jobs= -1)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q1(c)**   \n",
    "Repeat (b), for ensembles of size 10, 50, 100, 200 and 300 classifiers.  \n",
    "What level of improvement does this provide, in terms of percentage of instances correctly classified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trees = [10,50,100,200,300]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q1(d)**  \n",
    "Why does the level of improvement in accuracy often “level off” after an ensemble has been increased to a certain size?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q2(a)**  \n",
    "\n",
    "Load the Blood Alcohol Content (BAC) dataset using the CSV file provided.   \n",
    "This dataset contains a mix of numeric and categorical data, use one-hot encoding to convert to a numeric format. When this dataset was collected the BAC limit for driving was 0.8mg/ml. Convert this to a classification task by adding a binary Over/Under feature where Over is a BAC level > 0.8mg/ml. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "BAC_pd = pd.read_csv('BAC.csv')\n",
    "BAC_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q2(b)**  \n",
    "Using 10-fold cross validation, compare the performance of:  \n",
    "- a single decision tree, \n",
    "- a bagging ensemble (100 members) and \n",
    "- a boosting ensemble (also 100 members).\n",
    "\n",
    "**Q2(c)**  \n",
    "Are the results from a single cross validation run stable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "tree_bag = BaggingClassifier(dtree, \n",
    "                            n_estimators = 100,\n",
    "                            max_samples = 1.0, # bootstrap resampling \n",
    "                            bootstrap = True)\n",
    "\n",
    "GBC = GradientBoostingClassifier(n_estimators=100, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mean(el):\n",
    "    return np.array(el).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tree_l, bag_l, boost_l = [],[],[]\n",
    "reps = 1\n",
    "\n",
    "for rep in range(reps):\n",
    "    kf = KFold(n_splits=10, shuffle = True) # needed to ensure`shuffling   \n",
    "    tree_l.append(mean(cross_val_score(dtree, X, y, cv=kf, verbose = v, n_jobs = -1)))\n",
    "    bag_l.append(mean(cross_val_score(tree_bag, X, y, cv=kf, verbose = v, n_jobs = -1)))\n",
    "    boost_l.append(mean(cross_val_score(GBC, X, y, cv=kf, verbose = v, n_jobs = -1)))\n",
    "    \n",
    "print(\"Mean for Tree {:.2f}\".format(mean(tree_l)))\n",
    "print(\"Mean for Tree Bag {:.2f}\".format(mean(bag_l)))\n",
    "print(\"Mean for Tree Boost {:.2f}\".format(mean(boost_l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*With reps set to 1 we get quite different results from successive runs.*     \n",
    "**Q2(d)**  \n",
    "Repeat the 10-fold cross validation comparison 50 times to get a more robust comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*It seems both Bagging and Boosting improve over a single tree.  \n",
    "There is not much to choose between the two ensemble options.*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q3(a)**  \n",
    "Load the glass dataset from glass.csv.  \n",
    "Evaluate a 1-NN classifier using 10-fold cross-validation.   \n",
    "What is the overall accuracy achieved?\n",
    "\n",
    "**Q3(b)**   \n",
    "Apply bagging with a 1-NN classifier for an ensemble size of 100.  \n",
    "What is the improvement in terms of overall accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "glass_pd = pd.read_csv('glass.csv')\n",
    "glass_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y = glass_pd.pop('Type').values\n",
    "Xr = glass_pd.values\n",
    "\n",
    "gScal = StandardScaler().fit(Xr)\n",
    "X = gScal.transform(Xr)         # We are cheating on the scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kNN = KNeighborsClassifier(n_neighbors = 1)\n",
    "kNN_bag = BaggingClassifier(kNN, \n",
    "                            n_estimators = 100,\n",
    "                            max_samples = 1.0, # bootstrap resampling \n",
    "                            bootstrap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kNN_l, bag_l = [],[]\n",
    "reps = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q2(b)**  \n",
    "Now apply random subspacing with a 1-NN classifier for an ensemble size of 100.  \n",
    "How does it compare to the results from (b)? How do you explain this difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kNN_RS = BaggingClassifier(kNN, \n",
    "                            n_estimators = 100,\n",
    "                            max_features = 0.5,\n",
    "                            bootstrap = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kNN_l, bag_l, RS_l = [],[],[]\n",
    "reps = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Bagging doesn't achieve diversity with kNN whereas Random Subspacing does.*  \n",
    "**Q3(d)**  \n",
    "What happens to the overall ensemble accuracy when we increase the subspace size to a value closer to 1 (e.g. max_features=0.8)?  \n",
    "What is the explanation for the change in accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kNN_RS = BaggingClassifier(kNN, \n",
    "                            n_estimators = 100,\n",
    "                            max_features = 0.8,\n",
    "                            bootstrap = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RS_l = []\n",
    "reps = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "max_feature = 0.8 breaks Random Subspacing in this case. Presumably there is not enough diversity with 80% of features selected each time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}